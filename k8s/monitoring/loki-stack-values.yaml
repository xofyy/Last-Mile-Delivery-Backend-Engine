loki:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi

prometheus:
  enabled: true
  alertmanager:
    enabled: true
    persistentVolume:
      enabled: false
  server:
    persistentVolume:
      enabled: false
  # Load custom alert rules
  serverFiles:
    alerting_rules.yml:
      groups:
        - name: delivery-backend-alerts
          rules:
            - alert: HighErrorRate
              expr: |
                sum(rate(http_server_requests_seconds_count{status=~"5..", application="delivery-backend"}[5m]))
                /
                sum(rate(http_server_requests_seconds_count{application="delivery-backend"}[5m])) > 0.05
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "High error rate detected"
                description: "Error rate is {{ $value | humanizePercentage }}"
            
            - alert: HighLatencyP99
              expr: |
                histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket{application="delivery-backend"}[5m])) by (le)) > 1
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High P99 latency"
                description: "P99 latency is {{ $value }}s"
            
            - alert: PodCrashLooping
              expr: rate(kube_pod_container_status_restarts_total{namespace="default"}[15m]) * 60 * 15 > 3
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "Pod {{ $labels.pod }} is crash looping"
    
    # SLO Recording Rules
    recording_rules.yml:
      groups:
        - name: sli-recording-rules
          interval: 30s
          rules:
            # Availability SLI (5 min) - using job label or application label
            - record: sli:availability:ratio_5m
              expr: |
                sum(rate(http_server_requests_seconds_count{status!~"5.."}[5m]))
                /
                sum(rate(http_server_requests_seconds_count[5m]))
            
            # Availability SLI (1 hour)
            - record: sli:availability:ratio_1h
              expr: |
                sum(rate(http_server_requests_seconds_count{status!~"5.."}[1h]))
                /
                sum(rate(http_server_requests_seconds_count[1h]))
            
            # Latency SLI - requests under 500ms
            - record: sli:latency:ratio_500ms_5m
              expr: |
                sum(rate(http_server_requests_seconds_bucket{le="0.5"}[5m]))
                /
                sum(rate(http_server_requests_seconds_count[5m]))
            
            # P99 Latency
            - record: sli:latency:p99_5m
              expr: |
                histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket[5m])) by (le))
            
            # Error Budget Remaining (99.9% SLO target)
            - record: slo:availability:error_budget_remaining
              expr: |
                1 - ((1 - sli:availability:ratio_1h) / 0.001)
            
            # Burn Rate
            - record: slo:availability:burn_rate_1h
              expr: |
                (1 - sli:availability:ratio_1h) / 0.001
  
  # Alertmanager configuration
  # Slack webhook URL is loaded from Kubernetes Secret (see k8s/secrets/monitoring-external-secret.yaml)
  alertmanager:
    extraSecretMounts:
      - name: slack-webhook
        mountPath: /etc/alertmanager/secrets
        secretName: alertmanager-slack-webhook
        readOnly: true
  alertmanagerFiles:
    alertmanager.yml:
      global:
        # Reads from mounted secret file
        slack_api_url_file: '/etc/alertmanager/secrets/slack-webhook-url'
        resolve_timeout: 5m
      route:
        receiver: 'slack-notifications'
        group_by: ['alertname', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        routes:
          - match:
              severity: critical
            receiver: 'slack-critical'
            repeat_interval: 1h
      receivers:
        - name: 'slack-notifications'
          slack_configs:
            - channel: '#delivery-alerts'
              send_resolved: true
              title: '{{ .Status | toUpper }}: {{ .CommonLabels.alertname }}'
              text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        - name: 'slack-critical'
          slack_configs:
            - channel: '#delivery-oncall'
              send_resolved: true
              color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
              title: 'ðŸš¨ CRITICAL: {{ .CommonLabels.alertname }}'

promtail:
  enabled: true

grafana:
  enabled: true
  sidecar:
    datasources:
      enabled: true
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Tempo
          type: tempo
          access: proxy
          orgId: 1
          url: http://tempo.monitoring.svc.cluster.local:3200
          basicAuth: false
          isDefault: false
          version: 1
          editable: false
          apiVersion: 1
          uid: tempo
